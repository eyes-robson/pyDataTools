{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isaac Robson\n",
    "#### Demo for Gomez Lab, Department of Biomedical Engineering @ UNC - Chapel Hill\n",
    "#### For discovering genes with overlapping expression in barcodes\n",
    "\n",
    "#### Sept 2018\n",
    "\n",
    "When analyzing single-cell mRNA expression data, it's often handy to examine the occurence of genes mRNA sqeuence analysis. Current state of the art techniques for single-cell sequencing reveal only few hundred genes for each cell, so the overall frequency and co-occurence of genes is an interesting point of study. This notebook develops some functions to identify dublets, triplets, etc. of genes that exhibit presence in at least $min_shared_cells$ number of cells.\n",
    "\n",
    "This demo code was developed to help find insights for speedups in existing code in use by the Gomez Lab in the Department of Biomedical Engineering @ UNC - Chapel Hill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rely heavily on the frozensets, which we can use to represent combinations of genes handily. This allows us to wrap these sets into other data structures in Python for merging/accessing multiple sets at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'abv', 'acv', 'afg'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(frozenset(('acv',)) | frozenset(('abv',))) | (frozenset(('acv',)) | frozenset(('afg',))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of helper functions for gather_gene_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_candidate(tuple_of_previous_candidates, size_k):\n",
    "    return len(tuple_of_previous_candidates[0] | tuple_of_previous_candidates[1]) == size_k\n",
    "\n",
    "def big_valid_helper(chunk_of_candidates, size_k, q):\n",
    "    is_v_cand = ft.partial(is_valid_candidate, size_k = size_k)\n",
    "    out = set(filter(is_v_cand, chunk_of_candidates))\n",
    "    if(len(out) == 0):\n",
    "        q.put(None)\n",
    "    else:\n",
    "        q.put(out)\n",
    "    return\n",
    "   \n",
    "\n",
    "def get_next_valid_candidates(set_of_previous_candidates, n, size_k):\n",
    "    ''' Input: set of candidates from last iteration.\n",
    "        Returns: list of tuples of new valid candidates for this iteration.\n",
    "        Something something multiprocessing\n",
    "    '''\n",
    "\n",
    "    next_candidates = list(it.combinations(set_of_previous_candidates, 2))\n",
    "    \n",
    "    is_v_cand = ft.partial(is_valid_candidate, size_k = size_k)\n",
    "\n",
    "    if(n > pow(mp.cpu_count(),3)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        valid_candidates = p.map(is_v_cand, next_candidates)\n",
    "        valid_candidates = it.compress(next_candidates, valid_candidates)\n",
    "        p.close()\n",
    "    else:\n",
    "        valid_candidates = filter(is_v_cand, next_candidates)\n",
    "\n",
    "    return valid_candidates\n",
    "\n",
    "def check_new_candidate_count(tuple_of_new_candidates, last_candidate_cell_dict):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return len(intersect_candidate_cells(tuple_of_new_candidates, last_candidate_cell_dict))\n",
    "\n",
    "def union_candidates(tuple_of_new_candidates):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return tuple_of_new_candidates[0] | tuple_of_new_candidates[1]\n",
    "\n",
    "def check_greater(count, min_shared_cells):\n",
    "    return count > min_shared_cells\n",
    "\n",
    "def make_candidate_cell_dict(filtered_candidates, new_candidates, last_candidate_cell_dict):\n",
    "    '''combines dict entries for the filtered candidates set and returns a new candidate dict matching the new_candidates'''\n",
    "    combiner = ft.partial(intersect_candidate_cells, last_candidate_cell_dict)\n",
    "    if(len(new_candidates) > pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        out = dict(zip(new_candidates, list(map(combiner, filtered_candidates))))\n",
    "        p.close()\n",
    "        return out\n",
    "    else:\n",
    "        return dict(zip(new_candidates, list(map(combiner, filtered_candidates))))\n",
    "    \n",
    "def intersect_candidate_cells(filtered_candidates, last_candidate_cell_dict):\n",
    "    '''just a helper for make_candidate_cell_dict for mapping'''\n",
    "    return last_candidate_cell_dict[filtered_candidates[0]] & last_candidate_cell_dict[filtered_candidates[1]]\n",
    "    \n",
    "def expected_counter(tuple_of_new_candidates, last_candidate_cell_dict, total_cells):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return last_candidate_cell_dict[tuple_of_new_candidates[0]] * last_candidate_cell_dict[tuple_of_new_candidates[1]] / total_cells\n",
    "\n",
    "def generate_new_candidates(valid_candidates, last_candidate_cell_dict, min_shared_cells, total_cells):\n",
    "    '''heavy lifter. returns new candidates, new counts, new expected counts, and new candidate cell dicts'''\n",
    "    counter = ft.partial(check_new_candidate_count, last_candidate_cell_dict=last_candidate_cell_dict)\n",
    "    count_checker = ft.partial(check_greater,  min_shared_cells=min_shared_cells)\n",
    "\n",
    "    if(len(valid_candidates) >  pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        candidate_counts = list(p.map(counter, valid_candidates))\n",
    "        p.close()\n",
    "    else:\n",
    "        candidate_counts = list(map(counter, valid_candidates))\n",
    " \n",
    "    # it.compress and check_greater are fast enough\n",
    "    candidate_filter = list(map(count_checker, candidate_counts))\n",
    "    filtered_candidates = list(it.compress(valid_candidates, candidate_filter))\n",
    "    \n",
    "    if(len(filtered_candidates)==0):\n",
    "        print('Ran out of candidates! ')\n",
    "        return set(), set(), set(), {}\n",
    "    \n",
    "    # it.compress is fast enough\n",
    "    new_counts = list(it.compress(candidate_counts, candidate_filter))\n",
    "    \n",
    "    if(len(filtered_candidates) >  pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        new_expected_counts = list(p.map(expected_counter, filtered_candidates))\n",
    "        new_candidates = list(p.map(union_candidates, filtered_candidates))\n",
    "        p.close()\n",
    "    else:\n",
    "        new_expected_counts = list(map(expected_counter, filtered_candidates))\n",
    "        new_candidates = list(map(union_candidates, filtered_candidates))\n",
    "    \n",
    "    # parallelized locally\n",
    "    new_candidate_cell_dict = make_candidate_cell_dict(filtered_candidates, new_candidates, last_candidate_cell_dict)\n",
    "        \n",
    "    return new_candidates, new_counts, new_expected_counts, new_candidate_cell_dict\n",
    "\n",
    "### storage helpers ###\n",
    "def pickle_candidates(new_candidates, new_counts, new_expected_counts, new_candidate_cell_dict, size_k):\n",
    "    '''These files are gonna be decently big. Do not want to keep them in memory.'''\n",
    "    with open('candidates_' + str(size_k) + '.pickle', 'wb') as f:\n",
    "        pickle.dump(zip(list(new_candidates), list(new_counts), list(new_expected_counts)), f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('cell_dict_' + str(size_k) + '.pickle', 'wb') as f:\n",
    "        pickle.dump(new_candidate_cell_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def unpickle_candidates(size_k):\n",
    "    '''unpickler to unpickle the last one'''\n",
    "    with open('candidates_' + str(size_k) + '.pickle', 'rb') as f:\n",
    "        candidates, counts, expected_counts = zip(*pickle.load(f))\n",
    "    with open('cell_dict_' + str(size_k) + '.pickle', 'rb') as f:\n",
    "        candidate_cell_dict = pickle.load(f)\n",
    "        \n",
    "    return candidates, counts, expected_counts, candidate_cell_dict \n",
    "\n",
    "### these functions are for n_combinations large, e.g. when there are more candidates for evaluation than we'd like to run at the same time #\n",
    "def giant_filter(tuple_of_candidates, size_k, last_cell_dict, min_shared_cells):\n",
    "    if(not is_valid_candidate(tuple_of_candidates, size_k)):\n",
    "        return None\n",
    "    elif(len(intersect_candidate_cells(tuple_of_candidates, last_cell_dict)) <= min_shared_cells): \n",
    "        return None\n",
    "    else:\n",
    "        return tuple_of_candidates\n",
    "    \n",
    "def giant_candidater(tuple_of_valid_candidates, last_cell_dict, total_cells):\n",
    "    cand = union_candidates(tuple_of_valid_candidates)\n",
    "    new_cell_dict = {cand: intersect_candidate_cells(tuple_of_valid_candidates, last_cell_dict)}\n",
    "    count = len(new_cell_dict)\n",
    "    expected_count = last_cell_dict[tuple_of_valid_candidates[0]] * last_cell_dict[tuple_of_valid_candidates[1]] / total_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_gene_sets(tidy_df, min_shared_cells = 100, min_percent_cells = None, max_cluster_size = sys.maxsize):\n",
    "    '''\n",
    "        Input: tidy_df, a df with columns of ['symbol','barcode','gene name', 'class']\n",
    "        Output: Writes multiple files of format \"candidates_k.pickle\" and \"cell_dict_k.pickle\" for k in 1:max_cluster_size\n",
    "            \n",
    "        Assumptions:    barcode corresponds to cell_id\n",
    "                        gene_names <-> symbol (1:1 relationship)\n",
    "    '''\n",
    "    total_cells = tidy_df['barcode'].nunique()\n",
    "    \n",
    "    if(min_percent_cells is not None):\n",
    "        min_shared_cells = int(min_percent_cells * total_cells)\n",
    "    \n",
    "    # get the counts for each gene\n",
    "    candidate_cell_dict = tidy_df.groupby('symbol')\n",
    "    counts = candidate_cell_dict.count()['barcode'].to_dict()\n",
    "    candidate_cell_dict = candidate_cell_dict['barcode'].apply(frozenset)\n",
    "    \n",
    "    # store the symbols\n",
    "    next_candidates = set(counts.keys())\n",
    "\n",
    "    # filter stuff -- first iteration!\n",
    "    candidate_filter = list(map(lambda x: x > min_shared_cells, counts.values()))\n",
    "    candidates = list(map(frozenset, map(lambda x: (x,), it.compress(next_candidates, candidate_filter))))\n",
    "    counts = list(it.compress(counts.values(), candidate_filter))\n",
    "    \n",
    "    # clean the groups dict to get candidate cell sets for each input -- may be slow!\n",
    "    # print(frozenset(candidate_cell_dict.groups[next(iter(next(iter(candidates))))][0]),)\n",
    "    candidate_cell_dict = dict(zip(candidates,[candidate_cell_dict[next(iter(x))] for x in candidates]))\n",
    "    \n",
    "    # 0 expected counts for first iteration\n",
    "    expected_counts = [0] * len(candidate_cell_dict)\n",
    "    \n",
    "    # store our first entry!\n",
    "    size_k=1\n",
    "    pickle_candidates(candidates, counts, expected_counts, candidate_cell_dict, size_k)\n",
    "    \n",
    "    n = len(candidate_cell_dict)\n",
    "    print(str(n) + ' first candidates!')\n",
    "    \n",
    "    # loop!\n",
    "    while(n > 0 and size_k < max_cluster_size):\n",
    "        size_k += 1\n",
    "        \n",
    "        total_combos = pow(n,2) - n \n",
    "        \n",
    "        if(total_combos > pow(4096, 2)): # see if there would be > ~17 million combos\n",
    "            # break into chunks of ~ 4 million for evaluation\n",
    "            m = total_combos // pow(2048, 2)\n",
    "            carry_over = total_combos % pow(2048, 2)\n",
    "            \n",
    "            total_done = 0\n",
    "            ell = (m if carry_over == 0 else m + 1)\n",
    "            results = []\n",
    "            \n",
    "            valid_candidates_iter = it.combinations(candidates, 2)\n",
    "            print('created iter of size ' + str(total_combos))\n",
    "            stack = 0\n",
    "   \n",
    "            next_candidates = filter(None, [None])\n",
    "            \n",
    "            p = mp.Pool(processes = max(mp.cpu_count()-1,1))\n",
    "            \n",
    "            big_helper = ft.partial(giant_filter, size_k = size_k, last_cell_dict = candidate_cell_dict, min_shared_cells = min_shared_cells)\n",
    "            \n",
    "            while total_done < ell - 1:\n",
    "                next_candidates = it.chain(next_candidates, filter(lambda x: x is not None, p.map(big_helper, it.islice(valid_candidates_iter, pow(2048,2)))))\n",
    "                #print('iter chunk: ' + str(total_done) + ' of ' + str(ell-1))\n",
    "                total_done += 1\n",
    "                \n",
    "            next_candidates = it.chain(next_candidates, filter(lambda x: x is not None, p.map(big_helper, valid_candidates_iter)))\n",
    "            total_done += 1\n",
    "            \n",
    "            #print('generated next candidates!')\n",
    "            \n",
    "            big_cander = ft.partial(giant_candidater, last_cell_dict=candidate_cell_dict, total_cells=total_cells)\n",
    "            \n",
    "            chainer = iter([])\n",
    "            \n",
    "            while(total_done > 0):\n",
    "                chainer = it.chain(chainer, iter(p.map(big_cander, next_candidates)))\n",
    "                #print('remaining chunk: ' + str(total_done) + ' of ' + str(ell))\n",
    "                total_done -=  1\n",
    "            \n",
    "            try:\n",
    "                candidates, counts, expected_counts, candidate_cell_dict = zip(*chainer)\n",
    "            except:\n",
    "                print('Ran out of candidates here!')\n",
    "                break\n",
    "            candidate_cell_dict = {x:candidate_cell_dict[x] for x[0] in candidate_cell_dict}\n",
    "            \n",
    "            p.close()\n",
    "        else:\n",
    "            ### this is the easy logic section in case we have fewer combinations. Much easier to read\n",
    "            next_candidates = set(get_next_valid_candidates(candidates, n, size_k))\n",
    "            print('Generated next valid candidates! Size: ' + str(size_k))\n",
    "            print(len(next_candidates))\n",
    "            print(next(iter(next_candidates)))\n",
    "            candidates, counts, expected_counts, candidate_cell_dict = generate_new_candidates(next_candidates, candidate_cell_dict, min_shared_cells, total_cells)\n",
    "        \n",
    "        # calculate the new length of cells for the next iteration\n",
    "        n = len(candidate_cell_dict)\n",
    "        print('Evaluated '+ str(n) +' valid candidates!')\n",
    "            \n",
    "        # store our candidates at each stage. This allows us to reduce our RAM usage\n",
    "        if(n > 0):\n",
    "            pickle_candidates(candidates, counts, expected_counts, candidate_cell_dict, size_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "def generate_toy_data(n_cells, k_genes = 250):\n",
    "    ''' 26**3 genes ~ 17,500 unique genes\n",
    "        Lots of different cells - probably no collisions\n",
    "        \n",
    "        Inputs: k_genes -- the number of genes generated for each barcode (cell)\n",
    "    '''\n",
    "    join = lambda x: ''.join(x)\n",
    "    \n",
    "    genes = [0] * k_genes * n_cells\n",
    "    bars = [0] * k_genes * n_cells\n",
    "    classes = [0] * k_genes * n_cells\n",
    "    symbols = [0] * k_genes * n_cells\n",
    "    local_genes = [0] * k_genes\n",
    "    \n",
    "    for i in range(n_cells):\n",
    "        local_genes = [join(random.choices('abcdefghijklmnopqrstuvwxyz', k=3)) for x in range(k_genes)]\n",
    "        genes[i*k_genes:(i+1)*k_genes] = local_genes\n",
    "        bars[i*k_genes:(i+1)*k_genes] = [join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890', k=8))] * k_genes # sample a barcode\n",
    "        classes[i*k_genes:(i+1)*k_genes] = [join(random.choices('ABCD', k=1))] * k_genes # sample a class -- not accurate\n",
    "        symbols[i*k_genes:(i+1)*k_genes] = genes[i*k_genes:(i+1)*k_genes]\n",
    "    \n",
    "    dat = pd.DataFrame()\n",
    "    dat['genes'] = genes\n",
    "    dat['barcode'] = bars\n",
    "    dat['symbol'] = symbols\n",
    "    dat['class'] = classes\n",
    "         \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a toy dataset\n",
    "tidy_df = generate_toy_data(5000).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_df['barcode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidy_df['genes'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "4721 first candidates!\n",
      "created iter of size 22283120\n",
      "Ran out of candidates here!\n",
      "19.8 s ± 532 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "gather_gene_sets(tidy_df, min_shared_cells=75, max_cluster_size=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
