{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import itertools as it\n",
    "import functools as ft\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_candidate(tuple_of_previous_candidates, size_k):\n",
    "    return len(tuple_of_previous_candidates[0] | tuple_of_previous_candidates[1]) == size_k\n",
    "\n",
    "def big_valid_helper(chunk_of_candidates, size_k, q):\n",
    "    is_v_cand = ft.partial(is_valid_candidate, size_k = size_k)\n",
    "    out = set(filter(is_v_cand, chunk_of_candidates))\n",
    "    if(len(out) == 0):\n",
    "        q.put(None)\n",
    "    else:\n",
    "        q.put(out)\n",
    "    return\n",
    "   \n",
    "\n",
    "def get_next_valid_candidates(set_of_previous_candidates, n, size_k):\n",
    "    ''' Input: set of candidates from last iteration.\n",
    "        Returns: list of tuples of new valid candidates for this iteration.\n",
    "        Something something multiprocessing\n",
    "    '''\n",
    "\n",
    "    next_candidates = list(it.combinations(set_of_previous_candidates, 2))\n",
    "    \n",
    "    is_v_cand = ft.partial(is_valid_candidate, size_k = size_k)\n",
    "\n",
    "    if(n > pow(mp.cpu_count(),3)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        valid_candidates = p.map(is_v_cand, next_candidates)\n",
    "        valid_candidates = it.compress(next_candidates, valid_candidates)\n",
    "        p.close()\n",
    "    else:\n",
    "        valid_candidates = filter(is_v_cand, next_candidates)\n",
    "\n",
    "    return valid_candidates\n",
    "\n",
    "def check_new_candidate_count(tuple_of_new_candidates, last_candidate_cell_dict):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return len(intersect_candidate_cells(tuple_of_new_candidates, last_candidate_cell_dict))\n",
    "\n",
    "def union_candidates(tuple_of_new_candidates):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return tuple_of_new_candidates[0] | tuple_of_new_candidates[1]\n",
    "\n",
    "def check_greater(count, min_shared_cells):\n",
    "    return count > min_shared_cells\n",
    "\n",
    "def make_candidate_cell_dict(filtered_candidates, new_candidates, last_candidate_cell_dict):\n",
    "    '''combines dict entries for the filtered candidates set and returns a new candidate dict matching the new_candidates'''\n",
    "    combiner = ft.partial(intersect_candidate_cells, last_candidate_cell_dict=last_candidate_cell_dict)\n",
    "    if(len(new_candidates) > pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        out = dict(zip(new_candidates, list(map(combiner, filtered_candidates))))\n",
    "        p.close()\n",
    "        return out\n",
    "    else:\n",
    "        return dict(zip(new_candidates, list(map(combiner, filtered_candidates))))\n",
    "    \n",
    "def intersect_candidate_cells(filtered_candidates, last_candidate_cell_dict):\n",
    "    '''just a helper for make_candidate_cell_dict for mapping'''\n",
    "    return last_candidate_cell_dict[filtered_candidates[0]] & last_candidate_cell_dict[filtered_candidates[1]]\n",
    "    \n",
    "def expected_counter(tuple_of_new_candidates, last_candidate_cell_dict, total_cells):\n",
    "    '''takes in a tuple of new candidates, then evaluates if the intersection would have enough cells'''\n",
    "    return len(last_candidate_cell_dict[tuple_of_new_candidates[0]]) * len(last_candidate_cell_dict[tuple_of_new_candidates[1]]) / total_cells\n",
    "\n",
    "def generate_new_candidates(valid_candidates, last_candidate_cell_dict, min_shared_cells, total_cells):\n",
    "    '''heavy lifter. returns new candidates, new counts, new expected counts, and new candidate cell dicts'''\n",
    "    counter = ft.partial(check_new_candidate_count, last_candidate_cell_dict=last_candidate_cell_dict)\n",
    "    count_checker = ft.partial(check_greater,  min_shared_cells=min_shared_cells)\n",
    "    exp_checker = ft.partial(expected_counter, last_candidate_cell_dict=last_candidate_cell_dict, total_cells=total_cells)\n",
    "\n",
    "    s = time.time()\n",
    "    \n",
    "    if(len(valid_candidates) >  pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        candidate_counts = list(p.map(counter, valid_candidates))\n",
    "        p.close()\n",
    "    else:\n",
    "        candidate_counts = list(map(counter, valid_candidates))\n",
    "        \n",
    "    t = time.time()\n",
    "    \n",
    "    print(str(t - s) + ' seconds to count candidates')\n",
    "          \n",
    "    s = time.time()\n",
    " \n",
    "    # it.compress and check_greater are fast enough\n",
    "    candidate_filter = list(map(count_checker, candidate_counts))\n",
    "    filtered_candidates = list(it.compress(valid_candidates, candidate_filter))\n",
    "    \n",
    "    if(len(filtered_candidates)==0):\n",
    "        print('Ran out of candidates! ')\n",
    "        return set(), set(), set(), {}\n",
    "    \n",
    "    # it.compress is fast enough\n",
    "    new_counts = list(it.compress(candidate_counts, candidate_filter))\n",
    "          \n",
    "    t = time.time()\n",
    "    \n",
    "    print(str(t - s) + ' seconds to compress')\n",
    "    \n",
    "    s = time.time()\n",
    "          \n",
    "    if(len(filtered_candidates) >  pow(mp.cpu_count(),2)):\n",
    "        p = mp.Pool(processes=max(mp.cpu_count()-1,1))\n",
    "        new_expected_counts = list(p.map(exp_checker, filtered_candidates))\n",
    "        new_candidates = list(p.map(union_candidates, filtered_candidates))\n",
    "        p.close()\n",
    "    else:\n",
    "        new_expected_counts = list(map(exp_checker, filtered_candidates))\n",
    "        new_candidates = list(map(union_candidates, filtered_candidates))\n",
    "          \n",
    "    t = time.time()\n",
    "          \n",
    "    print(str(t - s) + ' seconds to count expected values')\n",
    "    \n",
    "    # parallelized locally\n",
    "    new_candidate_cell_dict = make_candidate_cell_dict(filtered_candidates, new_candidates, last_candidate_cell_dict)\n",
    "        \n",
    "    return new_candidates, new_counts, new_expected_counts, new_candidate_cell_dict\n",
    "\n",
    "### storage helpers ###\n",
    "def pickle_candidates(new_candidates, new_counts, new_expected_counts, new_candidate_cell_dict, size_k):\n",
    "    '''These files are gonna be decently big. Do not want to keep them in memory.'''\n",
    "    with open('candidates_' + str(size_k) + '.pickle', 'wb') as f:\n",
    "        pickle.dump(zip(list(new_candidates), list(new_counts), list(new_expected_counts)), f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('cell_dict_' + str(size_k) + '.pickle', 'wb') as f:\n",
    "        pickle.dump(new_candidate_cell_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def unpickle_candidates(size_k):\n",
    "    '''unpickler to unpickle the last one'''\n",
    "    with open('candidates_' + str(size_k) + '.pickle', 'rb') as f:\n",
    "        candidates, counts, expected_counts = zip(*pickle.load(f))\n",
    "    with open('cell_dict_' + str(size_k) + '.pickle', 'rb') as f:\n",
    "        candidate_cell_dict = pickle.load(f)\n",
    "        \n",
    "    return candidates, counts, expected_counts, candidate_cell_dict \n",
    "\n",
    "### these functions are for n_combinations large, e.g. when there are more candidates for evaluation than we'd like to run at the same time #\n",
    "def giant_filter(tuple_of_candidates, size_k, last_cell_dict, min_shared_cells):\n",
    "    if(not is_valid_candidate(tuple_of_candidates, size_k)):\n",
    "        return None\n",
    "    elif(len(intersect_candidate_cells(tuple_of_candidates, last_cell_dict)) <= min_shared_cells): \n",
    "        return None\n",
    "    else:\n",
    "        return tuple_of_candidates\n",
    "    \n",
    "def giant_candidater(tuple_of_valid_candidates, last_cell_dict, total_cells):\n",
    "    cand = union_candidates(tuple_of_valid_candidates)\n",
    "    new_cell_dict = {cand: intersect_candidate_cells(tuple_of_valid_candidates, last_cell_dict)}\n",
    "    count = len(new_cell_dict)\n",
    "    expected_count = last_cell_dict[tuple_of_valid_candidates[0]] * last_cell_dict[tuple_of_valid_candidates[1]] / total_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_gene_sets(tidy_df, min_shared_cells = 100, min_percent_cells = None, max_cluster_size = sys.maxsize):\n",
    "    '''\n",
    "        Input: tidy_df, a df with columns of ['symbol','barcode','gene name', 'class']\n",
    "        Output: Writes multiple files of format \"candidates_k.pickle\" and \"cell_dict_k.pickle\" for k in 1:max_cluster_size\n",
    "            \n",
    "        Assumptions:    barcode corresponds to cell_id\n",
    "                        gene_names <-> symbol (1:1 relationship)\n",
    "    '''\n",
    "    total_cells = tidy_df['barcode'].nunique()\n",
    "    \n",
    "    if(min_percent_cells is not None):\n",
    "        min_shared_cells = int(min_percent_cells * total_cells)\n",
    "    \n",
    "    # get the counts for each gene\n",
    "    candidate_cell_dict = tidy_df.groupby('symbol')\n",
    "    counts = candidate_cell_dict.count()['barcode'].to_dict()\n",
    "    candidate_cell_dict = candidate_cell_dict['barcode'].apply(frozenset)\n",
    "    \n",
    "    # store the symbols\n",
    "    next_candidates = counts.keys()\n",
    "\n",
    "    # filter stuff -- first iteration!\n",
    "    candidate_filter = list(map(lambda x: x > min_shared_cells, counts.values()))\n",
    "    candidates = list(map(frozenset, map(lambda x: (x,), it.compress(next_candidates, candidate_filter))))\n",
    "    counts = list(it.compress(counts.values(), candidate_filter))\n",
    "    \n",
    "    # clean the groups dict to get candidate cell sets for each input -- may be slow!\n",
    "    # print(frozenset(candidate_cell_dict.groups[next(iter(next(iter(candidates))))][0]),)\n",
    "    candidate_cell_dict = dict(zip(candidates,[candidate_cell_dict[next(iter(x))] for x in candidates]))\n",
    "    \n",
    "    # 0 expected counts for first iteration\n",
    "    expected_counts = [0] * len(candidate_cell_dict)\n",
    "    \n",
    "    # store our first entry!\n",
    "    size_k=1\n",
    "    pickle_candidates(candidates, counts, expected_counts, candidate_cell_dict, size_k)\n",
    "    \n",
    "    n = len(candidate_cell_dict)\n",
    "    print(str(n) + ' first candidates!')\n",
    "    \n",
    "    # loop!\n",
    "    while(n > 0 and size_k < max_cluster_size):\n",
    "        size_k += 1\n",
    "        \n",
    "        total_combos = pow(n,2) - n \n",
    "        \n",
    "        if(total_combos > pow(4096, 2)): # see if there would be > ~17 million combos\n",
    "            # break into chunks of ~ 4 million for evaluation\n",
    "            m = total_combos // pow(2048, 2)\n",
    "            carry_over = total_combos % pow(2048, 2)\n",
    "            \n",
    "            total_done = 0\n",
    "            ell = (m if carry_over == 0 else m + 1)\n",
    "            results = []\n",
    "            \n",
    "            valid_candidates_iter = it.combinations(candidates, 2)\n",
    "            print('created iter of size ' + str(total_combos))\n",
    "            stack = 0\n",
    "   \n",
    "            next_candidates = filter(None, [None])\n",
    "            \n",
    "            p = mp.Pool(processes = max(mp.cpu_count()-1,1))\n",
    "            \n",
    "            big_helper = ft.partial(giant_filter, size_k = size_k, last_cell_dict = candidate_cell_dict, min_shared_cells = min_shared_cells)\n",
    "            \n",
    "            while total_done < ell - 1:\n",
    "                next_candidates = it.chain(next_candidates, filter(lambda x: x is not None, p.map(big_helper, it.islice(valid_candidates_iter, pow(2048,2)))))\n",
    "                #print('iter chunk: ' + str(total_done) + ' of ' + str(ell-1))\n",
    "                total_done += 1\n",
    "                \n",
    "            next_candidates = it.chain(next_candidates, filter(lambda x: x is not None, p.map(big_helper, valid_candidates_iter)))\n",
    "            total_done += 1\n",
    "            \n",
    "            #print('generated next candidates!')\n",
    "            \n",
    "            big_cander = ft.partial(giant_candidater, last_cell_dict=candidate_cell_dict, total_cells=total_cells)\n",
    "            \n",
    "            chainer = iter([])\n",
    "            \n",
    "            while(total_done > 0):\n",
    "                chainer = it.chain(chainer, iter(p.map(big_cander, next_candidates)))\n",
    "                #print('remaining chunk: ' + str(total_done) + ' of ' + str(ell))\n",
    "                total_done -=  1\n",
    "            \n",
    "            try:\n",
    "                candidates, counts, expected_counts, candidate_cell_dict = zip(*chainer)\n",
    "            except:\n",
    "                print('Ran out of candidates here!')\n",
    "                break\n",
    "            candidate_cell_dict = {x:candidate_cell_dict[x] for x[0] in candidate_cell_dict}\n",
    "            \n",
    "            p.close()\n",
    "        else:\n",
    "            ### this is the easy logic section in case we have fewer combinations. Much easier to read\n",
    "            next_candidates = set(get_next_valid_candidates(candidates, n, size_k))\n",
    "            print('Generated next valid candidates! Size: ' + str(size_k))\n",
    "            print(len(next_candidates))\n",
    "            print(next(iter(next_candidates)))\n",
    "            candidates, counts, expected_counts, candidate_cell_dict = generate_new_candidates(next_candidates, candidate_cell_dict, min_shared_cells, total_cells)\n",
    "        \n",
    "        # calculate the new length of cells for the next iteration\n",
    "        n = len(candidate_cell_dict)\n",
    "        print('Evaluated '+ str(n) +' valid candidates!')\n",
    "            \n",
    "        # store our candidates at each stage. This allows us to reduce our RAM usage\n",
    "        if(n > 0):\n",
    "            pickle_candidates(candidates, counts, expected_counts, candidate_cell_dict, size_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('./cord_blood_kinases.csv', sep=',', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3455"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(dat[dat['symbol']=='NRBP1']['barcode'].values) & set(dat[dat['symbol']=='LYN']['barcode'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    candidate_cell_dict = tidy_df.groupby('symbol')\n",
    "    counts = candidate_cell_dict.count()['barcode'].to_dict()\n",
    "    candidate_cell_dict = candidate_cell_dict['barcode'].apply(frozenset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32942"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby('symbol')['barcode'].count().to_dict()['NRBP1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barcode</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1</td>\n",
       "      <td>STK4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1</td>\n",
       "      <td>RIOK3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1</td>\n",
       "      <td>PDK2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1</td>\n",
       "      <td>MAPK7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1</td>\n",
       "      <td>CDK10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                barcode symbol\n",
       "1  MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1   STK4\n",
       "2  MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1  RIOK3\n",
       "3  MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1   PDK2\n",
       "4  MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1  MAPK7\n",
       "5  MantonCB1_HiSeq_1-AAACCTGCACAGACAG-1  CDK10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531681, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 first candidates!\n",
      "Generated next valid candidates! Size: 2\n",
      "45753\n",
      "(frozenset({'HCK'}), frozenset({'RIPK1'}))\n",
      "50.04134488105774 seconds to count candidates\n",
      "0.012686014175415039 seconds to compress\n",
      "47.803678035736084 seconds to count expected values\n",
      "Evaluated 5182 valid candidates!\n",
      "created iter of size 26847942\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "gather_gene_sets(dat, min_percent_cells=.01)\n",
    "t = time.time()\n",
    "\n",
    "print(str(t-s) + ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
